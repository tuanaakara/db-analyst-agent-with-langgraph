"""
Service module for managing interactions with the Google Gemini API.

This module contains the `GeminiService` class, which is responsible for
connecting to the Gemini service using the LangChain library, sending
requests, and enabling integration with tools.
"""
from . import config
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
import logging

logger = logging.getLogger(__name__)

# Load environment variables from the .env file
load_dotenv()

class GeminiService:
    """
    A dedicated service class for interacting with the Google Gemini LLM.

    This class uses LangChain's ChatGoogleGenerativeAI module to send
    requests to the specified Gemini model and receive responses.
    """

    def __init__(self):
        """
        Initializes the GeminiService.
        The model name and API key are read directly from environment variables.

        Args:
            temperature (float): The creativity level of the model (between 0 and 1).
            max_tokens (int): The maximum number of tokens to receive from the model.
        """
        self.model_name = config.GEMINI_MODEL_NAME
        self.api_key = config.GEMINI_API_KEY
        self.temperature = config.LLM_TEMPERATURE
        self.max_tokens = config.LLM_MAX_TOKENS

        # Initialize LangChain's Google client
        self.llm = ChatGoogleGenerativeAI(
            model=config.GEMINI_MODEL_NAME,
            temperature=config.LLM_TEMPERATURE,
            max_output_tokens=config.LLM_MAX_TOKENS,
            google_api_key=config.GEMINI_API_KEY,
        )
        logger.info("✅ LLM Service '%s' modeli ile başarıyla başlatıldı.", self.model_name)

    def get_response(self, prompt: str) -> str:
        """
        Gets a text response from the LLM for a given prompt.

        Args:
            prompt (str): The text to be sent to the LLM.

        Returns:
            str: The text response generated by the model.
        
        Raises:
            Exception: If an error occurs while receiving a response from the LLM.
        """
        try:
            response = self.llm.invoke(prompt)
            return response.content
        except Exception as e:
            # exc_info=True, hatanın tüm detaylarını (traceback) loga ekler.
            logger.error("❌ LLM'den yanıt alınırken hata oluştu: %s", e, exc_info=True)
            raise e

    def get_llm_with_tools(self, tools: list) -> any:
        """
        Binds the LLM with LangChain tools and returns a new runnable object.

        Args:
            tools (list): A list of LangChain tools to be bound to the LLM.

        Returns:
            any: An invokable LangChain object bound with tools.
        """
        return self.llm.bind_tools(tools)